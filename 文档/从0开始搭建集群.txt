HBase key-value:(ImmutableBytesWritable, Result) table类型：TableInputFormat
HDFS  key-value:(LongWritable,Text)  输出格式：TextInputFormat



安装完vmware12和centos7.4后
1.设置静态ip：	vi /etc/sysconfig/network-scripts/ifcfg-ens33
	修改BOOTPROTO=static
	修改ONBOOT=yes
	添加：
	IPADDR=192.168.195.101
	GATEWAY=192.168.195.2
	DNS1=192.168.195.2
2.修改完重启服务配置文件才能生效：	systemctl restart network 
  测试是否配置成功：ping www.baidu.com 
  
3.修改主机名：	hostnamectl set-hostname hadoop101 
  配置hosts映射文件：		vi /etc/hosts
  修改window7的主机映射文件（hosts文件）	进入C:\Windows\System32\drivers\etc路径

4.关闭防火墙：	systemctl stop firewalld
  开启防火墙：	systemctl start firewalld
  关闭开机启动：	systemctl disable firewalld

5.临时和永久关闭Selinux
	临时关闭：
	[root@localhost ~]# getenforce
	Enforcing
	[root@localhost ~]# setenforce 0
	[root@localhost ~]# getenforce
	Permissive
	永久关闭：
	[root@localhost ~]# vi /etc/sysconfig/selinux
	SELINUX=enforcing 改为 SELINUX=disabled
	重启服务  reboot

以上步骤都完成以后，克隆当前虚拟机，内存和cup根据个人电脑配置自行修改，修改ip、主机名、映射

6.安装vim: 		yum install -y vim*
  安装rsync: 	yum install -y rsync 
  安装lrzsz: 	yum install -y lrzsz
  安装ntp:		yum install -y ntp
  yum install -y vim* rsync lrzsz ntp
7.ssh免密登陆： ssh-keygen -t rsa
			   ssh-copy-id hadoop102
    ===========================================================
    #ssh免密登陆脚本
	#! /bin/bash
	function sshFreeLogin()
	{
	 #1.检测expect服务是否存在，不存在则使用yum安装expect
	 expectIsExists=`rpm -qa | grep expect` 
	 #-z string的长度为0则为真
	 if [ -z $expectIsExists ]
	 then
	      yum -y install expect
	 fi
	 #2.密钥对不存在则创建密钥
	 [ ! -f /root/.ssh/id_rsa.pub ] && ssh-keygen -t rsa -P "" -f /root/.ssh/id_rsa
	 while read line;do
	       #提取文件中的ip
	       hostname=`echo $line | cut -d " " -f2`
	       #提取文件中的用户名
	       user_name=`echo $line | cut -d " " -f3`
	       #提取文件中的密码
	       pass_word=`echo $line | cut -d " " -f4`          
	       expect <<EOF
	              #复制公钥到目标主机
	              spawn ssh-copy-id $hostname
	              expect {
	                      #expect实现自动输入密码
	                      "yes/no" { send "yes\n";exp_continue } 
	                      "password" { send "$pass_word\n";exp_continue }
	                      eof
	              }
	EOF
	 # 读取存储ip的文件，host_ip文件所在的目录地址
	 done < /host_ip.txt
	}
	sshFreeLogin


	#host_ip.txt ip 映射 账号 密码
	192.168.195.101 hadoop101 root 123456
	192.168.195.102 hadoop102 root 123456
	192.168.195.103 hadoop103 root 123456
    ===========================================================
  xsync分发脚本：
	===========================================================
		#!/bin/bash
		#1 获取输入参数个数，如果没有参数，直接退出
		pcount=$#
		if((pcount==0)); then
		echo no args;
		exit;
		fi

		#2 获取文件名称
		p1=$1
		fname=`basename $p1`
		echo filename=$fname

		#3 获取上级目录到绝对路径
		pdir=`cd -P $(dirname $p1); pwd`
		echo pdir=$pdir

		#4 获取当前用户名称
		user=`whoami`

		#5 循环
		for((host=102; host<104; host++)); do
		        echo ------------------- hadoop$host --------------
		        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir
		done
	===============================================================
  ntp时间同步：
    ===============================================================
    1.服务器设置:192.168.159.101

	rpm -qa|grep ntp
	yum install -y ntp
	systemctl start ntpd
	systemctl enable ntpd
	systemctl is-enabled ntpd 查看是否开机启动

	vim /etc/ntp.conf
	1.注释 server (0,1,2,3).centos.pool.ntp.org iburst
	2.添加 server 172.127.1.0
	       fudge  172.127.1.0 startum 10       
	       server 192.168.1.100 prefer

	vim /etc/sysconfig/ntpd
	SYNC_HWCLOCK=yes

	===============================================
	#快捷操作
	sed -i 's/server [0-3]/#&/' /etc/ntp.conf
	echo 'SYNC_HWCLOCK=yes' >> /etc/sysconfig/ntpd

	cat >> /etc/ntp.conf <<EOF
	server 172.127.1.0
	fudge 172.127.1.0 startum 10
	server 192.168.1.100 prefer
	EOF
	===============================================

	2.客户端设置:192.168.159.102       192.168.159.103

	rpm -qa|grep ntp
	yum install -y ntp
    systemctl start ntpd
	systemctl enable ntpd

	vim /etc/ntp.conf
	1.注释 server (0,1,2,3).centos.pool.ntp.org iburst
	2.添加 server 192.168.159.101 prefer

	从服务器ntpConf.sh脚本
	===================================================
	#ntpConf.sh脚本
	#!bin/bash
	user=`whoami`
	for((host=102; host<104; host++)); do
		#1.注释server 0到server 3内容
		ssh $user@hadoop$host "sed -i 's/server [0-3]/#&/' /etc/ntp.conf"
		#2.添加"server 192.168.159.101 prefer"到各个从服务器
	    ssh $user@hadoop$host "echo 'server 192.168.159.101 prefer' >> /etc/ntp.conf"
	done
	===================================================
	
	查看同步状态：
	ntpstat
	ntpq -p
    ===============================================================
8.安装jdk和hadoop:	tar -zxvf /opt/software/jdk-8u201-linux-x64.tar.gz -C /opt/module/
					tar -zxvf /opt/software/hadoop-2.7.2.tar.gz -C /opt/module/
		    		vim /etc/profile,末尾加以下文件（shitf+g）:
=======================================================
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_201
export PATH=$PATH:$JAVA_HOME/bin

#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-2.7.2
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
========================================================
			source /etc/profile 让文件生效
			xsync /opt/module
			xsync /etc/profile
			其余节点执行: source /etc/profile
						 java -version
						 hadoop version

9.  配置8个文件，都在$HADOOP_HOME/etc/hadoop
9.1 首先配置hadoop-env.sh,yarn-env.sh,mapred-env.sh文件,配置JAVA_HOME
    在每个文件第二行添加 export JAVA_HOME=/opt/module/jdk1.8.0_201

9.2 配置Core-site.xml
        <!-- 指定HDFS中NameNode的地址 -->
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop101:9000</value>
        </property>

        <!-- 指定Hadoop运行时产生文件的存储目录 -->
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/opt/module/hadoop-2.7.2/data/tmp</value>
        </property>
        
9.3 配置hdfs-site.xml
        <!-- 数据的副本数量 -->
        <property>
            <name>dfs.replication</name>
            <value>3</value>
        </property>
        <!-- 指定Hadoop辅助名称节点主机配置 -->
        <property>
              <name>dfs.namenode.secondary.http-address</name>
              <value>hadoop103:50090</value>
        </property>
9.4 配置yarn-site.xml
        <!-- Site specific YARN configuration properties -->
        <!-- Reducer获取数据的方式 -->
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>

        <!-- 指定YARN的ResourceManager的地址 -->
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>hadoop102</value>
        </property>
        <!-- 日志聚集功能 -->
        <property>
            <name>yarn.log-aggregation-enable</name>
            <value>true</value>
        </property>

        <!-- 日志保留时间设置7天 -->
        <property>
            <name>yarn.log-aggregation.retain-seconds</name>
            <value>604800</value>
        </property>

9.5 配置mapred-site.xml
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop103:10020</value>
    </property>
    <!-- 历史服务器web端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop103:19888</value>
    </property>

9.6 配置slaves
    hadoop101
    hadoop102
    hadoop103

9.7 分发配置文件:	   xsync /opt/module/hadoop-2.7.2/etc

9.8 启动集群
=====================================================
#util显示jps
#!/bin/bash

hosts="hadoop101 hadoop102 hadoop103"
for host in $hosts
do
    echo "==========     $host     =========="
    ssh $host "source /etc/profile;jps"
done
=====================================================
=========================================================================================================================
#zk.sh  cat /etc/profile >> ~/.bashrc
#!/bin/bash
case $1 in
"start"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper-3.4.10/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper-3.4.10/bin/zkServer.sh status"
	done
};;
esac

#startcluster.sh集群启动脚本
#!/bin/bash
echo "==========     开始开启所有节点服务       =========="

echo "==========     正在启动Zookeeper          =========="
hosts="hadoop101 hadoop102 hadoop103"
for host in $hosts
do
    ssh $host "source /etc/profile;/opt/module/zookeeper-3.4.10/bin/zkServer.sh start"
done

echo "==========     正在启动HDFS               =========="
ssh hadoop101 "/opt/module/hadoop-2.7.2/sbin/start-dfs.sh"

echo "==========     正在启动YARN               =========="
ssh hadoop102 "/opt/module/hadoop-2.7.2/sbin/start-yarn.sh"

echo "==========     正在启动JobHistoryServer   =========="
ssh hadoop103  "/opt/module/hadoop-2.7.2/sbin/mr-jobhistory-daemon.sh start historyserver"


#stopcluster.sh集群关闭脚本
#!/bin/bash
echo "==========     开始关闭所有节点服务       =========="

echo "==========     正在关闭Zookeeper          =========="
hosts="hadoop101 hadoop102 hadoop103"
for host in $hosts
do
    ssh $host "source /etc/profile;/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop"
done

echo "==========     正在关闭HDFS               =========="
ssh hadoop101 "/opt/module/hadoop-2.7.2/sbin/stop-dfs.sh"

echo "==========     正在关闭YARN               =========="
ssh hadoop102 "/opt/module/hadoop-2.7.2/sbin/stop-yarn.sh"

echo "==========     正在关闭JobHistoryServer   =========="
ssh hadoop103  "/opt/module/hadoop-2.7.2/sbin/mr-jobhistory-daemon.sh stop historyserver"


=========================================================================================================================
cd /opt/module/hadoop-2.7.2
如果集群是第一次启动，需要格式化NameNode，注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据
第一次启动格式化集群： bin/hdfs namenode -format
hadoop101启动HDFS：	 sbin/start-dfs.sh
hadoop102启动YARN：	 sbin/start-yarn.sh
启动JobHistory：		 mr-jobhistory-daemon.sh start historyserver
查看HDFS文件系统:     hadoop101:50070
查看JobHistory:		 hadoop103:19888

10.安装zookeeper
安装目录创建zkData目录，在zkData下创建myid文件
zoo.cfg文件：1.修改dataDir目录，在zkData路径
			2.添加 
			server.1=hadoop101:2888:3888
			server.2=hadoop102:2888:3888
			server.3=hadoop103:2888:3888


ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/hbase-1.3.1/conf/core-site.xml
ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase-1.3.1/conf/hdfs-site.xml

11.安装HBase

启动Zookeeper集群
启动Hadoop集群

HBase的解压： tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module
HBase的配置文件
1）hbase-env.sh修改内容：
export JAVA_HOME=/opt/module/jdk1.8.0_144
export HBASE_MANAGES_ZK=false  //不启用hbase自带的zookeeper实例
2）hbase-site.xml修改内容：
<configuration>
	<property>     
		<name>hbase.rootdir</name>     
		<value>hdfs://hadoop101:9000/hbase</value>   
	</property>

	<property>   
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>

   <!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->
	<property>
		<name>hbase.master.port</name>
		<value>16000</value>
	</property>

	<property>   
		<name>hbase.zookeeper.quorum</name>
	     <value>hadoop101:2181,hadoop102:2181,hadoop103:2181</value>
	</property>

	<property>   
		<name>hbase.zookeeper.property.dataDir</name>
	     <value>/opt/module/zookeeper-3.4.10/zkData</value>
	</property>
</configuration>
3）regionservers：
hadoop101
hadoop102
hadoop103

4）软连接hadoop配置文件到hbase：
ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/hbase-1.3.1/conf/core-site.xml
ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase-1.3.1/conf/hdfs-site.xml
HBase远程发送到其他集群: xsync hbase-1.3.1/


高可用
1．关闭HBase集群（如果没有开启则跳过此步）
[atguigu@hadoop101 hbase-1.3.1]$ bin/stop-hbase.sh
2．在conf目录下创建backup-masters文件
[atguigu@hadoop101 hbase-1.3.1]$ touch conf/backup-masters
3．在backup-masters文件中配置高可用HMaster节点
[atguigu@hadoop101 hbase-1.3.1]$ echo hadoop102 >> conf/backup-masters
4．将整个conf目录scp到其他节点
[atguigu@hadoop101 hbase-1.3.1]$ xsycn conf/backup-masters
5．打开页面测试查看
http://hadooo101:16010 


			
    